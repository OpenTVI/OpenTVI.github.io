
<head>
    <meta charset="utf-8">
    <title>TVI · Resources</title>
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <meta content="" name="keywords">
    <meta content="" name="description">

    <!-- Favicon -->
    <link href="img/logo_fav.png" rel="icon">

    <!-- Google Web Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Heebo:wght@400;500;600&family=Nunito:wght@600;700;800&family=Pacifico&display=swap" rel="stylesheet">

    <!-- Icon Font Stylesheet -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.10.0/css/all.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.4.1/font/bootstrap-icons.css" rel="stylesheet">

    <!-- Libraries Stylesheet -->
    <link href="lib/animate/animate.min.css" rel="stylesheet">
    <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">
    <link href="lib/tempusdominus/css/tempusdominus-bootstrap-4.min.css" rel="stylesheet" />

    <!-- Customized Bootstrap Stylesheet -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Template Stylesheet -->
    <link href="css/style.css" rel="stylesheet">
    <style>
        .zoom-image {
            width: 100%; /* 图片占满容器宽度 */
            height: auto; /* 高度自适应 */
            transition: transform 0.4s ease; /* 设置变换的过渡效果 */
        }
    
        /* 鼠标悬停在图片容器上时应用的放大效果 */
        .col-lg-3:hover .zoom-image,
        .col-sm-3:hover .zoom-image {
            transform: scale(3); /* 鼠标悬停时将图片放大20% */
        }
    
        .divider {
            border: none;
            border-top: 1px solid #141313; /* 灰色分割线 */
            margin: 10px 0; /* 设置上下间距 */
        }
    </style>
    
</head>

<body>
    <div class="container-xxl bg-white p-0">
        <!-- Spinner Start -->
        <div id="spinner" class="show bg-white position-fixed translate-middle w-100 vh-100 top-50 start-50 d-flex align-items-center justify-content-center">
            <div class="spinner-border text-primary" style="width: 3rem; height: 3rem;" role="status">
                <span class="sr-only">Loading...</span>
            </div>
        </div>
        <!-- Spinner End -->



        <!-- Navbar & Hero Start -->
        <div class="container-xxl position-relative p-0">
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark px-4 px-lg-5 py-3 py-lg-0">
                <a href="" class="navbar-brand p-0">
                    <!-- <h1 class="text-primary m-0"><i class="fa fa-utensils me-3"></i>IIP-XDU</h1> -->
                    <img src="img/logo.png" alt="Logo" >
                </a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse">
                    <span class="fa fa-bars"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarCollapse">
                    <div class="navbar-nav ms-auto py-0 pe-4">
                        <a href="index.html" class="nav-item nav-link">Home</a>
                        <a href="team.html" class="nav-item nav-link">Team</a>
                        <a href="research.html" class="nav-item nav-link">Research</a>
                        <a href="resources.html" class="nav-item nav-link active">Resources</a>
                        <!--
                        <div class="nav-item dropdown">
                            <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown">Projects</a>
                            <div class="dropdown-menu m-0">
                                <a href="booking.html" class="dropdown-item">XAI</a>
                                <a href="team.html" class="dropdown-item">AIGC</a>
                                <a href="testimonial.html" class="dropdown-item">HIT</a>
                            </div>
                        </div>
                      -->
                         
                    </div>
                    <!-- <a href="" class="nav-item nav-link"><i class="fa fa-2x fa-language text-primary "></i></a> -->
                </div>
            </nav>

            <div class="container-xxl py-5 bg-dark hero-header mb-5">
                <div class="container text-center my-5 pt-5 pb-4">
                    <h1 class="display-3 text-white mb-3 animated slideInDown">Video Intelligent Analysis</h1>
                    <nav aria-label="breadcrumb">
                        <ol class="breadcrumb justify-content-center text-uppercase">
                            <li class="breadcrumb-item"><a href="index.html">Home</a></li>
                            <li class="breadcrumb-item"><a href="https://github.com/OpenTVI/OpenTVI.github.io">Github</a></li>
                            <li class="breadcrumb-item text-white active" aria-current="page">Resources</li>
                        </ol>
                    </nav>
                </div>
            </div>
        </div>
        <!-- Navbar & Hero End -->

        <!-- About Start -->
        <div class="container-xxl py-5">
            <div class="container">
                <div class="row g-5 align-items-center">

                    <div class="col-lg-8-1">
                        <h5 class="section-title ff-secondary text-start text-primary fw-normal">Video Intelligent Analysis</h5>
                        <p class="mb-4">Video Intelligent Analysis focuses on the comprehensive analysis of content captured through video surveillance, providing essential support for the reliable "Smart Mind and Super Vision · Fire Eye" video monitoring system. This system plays a pivotal role in ensuring the effectiveness and accuracy of surveillance by leveraging advanced AI technologies to detect, recognize, and track objects and individuals across different environments. Our research spans several key areas, including Small Object Detection, which aims to identify and track objects that are typically hard to detect due to their size; Pedestrian Detection, which focuses on detecting pedestrians in diverse settings; Face Super-Resolution, designed to enhance the resolution and clarity of facial images for better identification; and Person Re-Identification, which seeks to match individuals across multiple camera views, regardless of variations in lighting, posture, or background.</p>
                    
                    </div>
                </div>
            </div>
        </div>
        <!-- About End -->

         
        <!-- Menu Start -->

        <div class="container-xxl py-5">
            <div class="container">
              <!--
                <div class="text-center wow fadeInUp" data-wow-delay="0.1s">
                    <h5 class="section-title ff-secondary text-center text-primary fw-normal">Resources</h5>
                    <h1 class="mb-5">Most Popular Items</h1>
                </div>
                -->
                <div class="tab-class text-center wow fadeInUp" data-wow-delay="0.1s">


                    <div class="tab-content">

                        <div id="tab-1" class="tab-pane fade show p-0 active">
                            <div class="row g-4">
                                <!--单个论文开始1-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/dsreid.png" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>Dual-Space Video Person Re-identification</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://link.springer.com/article/10.1007/s11263-025-02350-5" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng,  Changjiang Kuang, Shuang Li, Ji Gan, Haosheng Chen and Xinbo Gao*
                                            <hr class="divider">
                                            <p>
                                                Video person re-identification (VReID) aims to recognize individuals across video sequences. Existing methods primarily use Euclidean space for representation learning but struggle to capture complex hierarchical structures, especially in scenarios with occlusions and background clutter. In contrast, hyperbolic space, with its negatively curved geometry, excels at preserving hierarchical relationships and enhancing discrimination between similar appearances. Inspired by these, we propose Dual-Space Video Person Re-Identification (DS-VReID) to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features while also exploring the intrinsic hierarchical relations, thereby enhancing the discriminative capacity of the features. Specifically, we design the Dynamic Prompt Graph Construction (DPGC) module, which uses a pre-trained CLIP model with learnable dynamic prompts to construct 3D graphs that capture subtle changes and dynamic information in video sequences. Building upon this, we introduce the Hyperbolic Disentangled Aggregation (HDA) module, which addresses long-range dependency modeling by decoupling node distances and integrating adjacency matrices, capturing detailed spatial-temporal hierarchical relationships.
                                            </p>
                                            <span class="fst-italic">International Journal of Computer Vision (IJCV), 2025</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始1-->

                                <!--单个论文开始2-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/detr.png" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>Selecting Learnable Training Samples is All DETRs Need in Crowded Pedestrian Detection</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://dl.acm.org/doi/abs/10.1145/3581783.3612189" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Feng Gao,  Jiaxu Leng*, Ji Gan, and Xinbo Gao*
                                            <hr class="divider">
                                            <p>
                                                DEtection TRansformer (DETR) and its variants (DETRs) achieved impressive performance in general object detection. However, in crowded pedestrian detection, the performance of DETRs is still unsatisfactory due to the inappropriate sample selection method which results in more false positives. To settle the issue, we proposed a simple but effective sample selection method for DETRs, Sample Selection for Crowded Pedestrians (SSCP), which consists of the constraint-guided label assignment scheme (CGLA) and the utilizability-aware focal loss (UAFL). Our core idea is to select learnable samples for DETRs and adaptively regulate the loss weights of samples based on their utilizability. Specifically, in CGLA, we proposed a new cost function to ensure that only learnable positive training samples are retained and the rest are negative training samples. Further, considering the utilizability of samples, we designed UAFL to adaptively assign different loss weights to learnable positive samples depending on their gradient ratio and IoU. 
                                            </p>
                                            <span class="fst-italic">ACM International Conference on Multimedia (ACM MM), 2023</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始2-->


                                <!--单个论文开始3-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/aerial_detection.png" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>Recent Advances for Aerial Object Detection: A Survey</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://dl.acm.org/doi/abs/10.1145/3664598" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng,  Yongming Ye, Mengjingcheng Mo, Chengqiang Gao, Ji Gan, Bin Xiao and Xinbo Gao*
                                            <hr class="divider">
                                            <p>
                                                Aerial object detection, as object detection in aerial images captured from an overhead perspective, has been widely applied in urban management, industrial inspection, and other aspects. However, the performance of existing aerial object detection algorithms is hindered by variations in object scales and orientations attributed to the aerial perspective. This survey presents a comprehensive review of recent advances in aerial object detection. We start with some basic concepts of aerial object detection and then summarize the five imbalance problems of aerial object detection, including scale imbalance, spatial imbalance, objective imbalance, semantic imbalance, and class imbalance. Moreover, we classify and analyze relevant methods and especially introduce the applications of aerial object detection in practical scenarios. Finally, the performance evaluation is presented on two popular aerial object detection datasets VisDrone-DET and DOTA, and we discuss several future directions that could facilitate the development of aerial object detection.
                                            </p>
                                            <span class="fst-italic">ACM Computing Surveys, 2024</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始3-->

                                <!--单个论文开始4-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/icnet.png" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>ICNet: Joint Alignment and Reconstruction via Iterative Collaboration for Video Super-Resolution</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://dl.acm.org/doi/abs/10.1145/3503161.3547994" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng,  Jia Wang, Xinbo Gao*, Bo Hu, Chengqiang Gao
                                            <hr class="divider">
                                            <p>
                                                Most previous frameworks either cost too much time or adopt some fixed modules resulting in alignment error in video super-resolution (VSR). In this paper, we propose a novel many-to-many VSR framework with Iterative Collaboration (ICNet), which employs the concurrent operation by iterative collaboration between alignment and reconstruction proving to be more efficient and effective than existing recurrent and sliding-window frameworks. With the proposed iterative collaboration, alignment can be conducted on super-resolved features from reconstruction while accurate alignment boosts reconstruction in return. In each iteration, the features of low-resolution video frames are first fed into the alignment and reconstruction subnetworks, which can generate temporal aligned features and spatial super-resolved features. Then, both outputs are fed into the proposed Tidy Two-stream Fusion (TTF) subnetwork that shares inter-frame temporal information and intra-frame spatial information without redundancy. Moreover, we design the Frequency Separation Reconstruction (FSR) subnetwork to not only model high-frequency and low-frequency information separately but also take benefit of each other for better reconstruction.  
                                            </p>
                                            <span class="fst-italic">ACM International Conference on Multimedia (ACM MM), 2022</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始4-->

                                <!--单个论文开始5-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/DGVDL.png" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>Difficulty-Guided Variant Degradation Learning for Blind Image Super-Resolution</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/document/10709871" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng,  Jia Wang, Mengjingcheng Mo,Ji Gan, Wen Lu and Xinbo Gao*
                                            <hr class="divider">
                                            <p>
                                                Recent blind super-resolution (BSR) methods are explored to handle unknown degradations and achieve impressive performance. However, the prevailing assumption in most BSR methods is the spatial invariance of degradation kernels across the entire image, which leads to significant performance declines when faced with spatially variant degradations caused by object motion or defocusing. Additionally, these methods do not account for the human visual system’s tendency to focus differently on areas of varying perceptual difficulty, as they uniformly process each pixel during reconstruction. To cope with these issues, we propose a difficulty-guided variant degradation learning network for BSR, named difficulty-guided degradation learning (DDL)-BSR, which explores the relationship between reconstruction difficulty and degradation estimation. Accordingly, the proposed DDL-BSR consists of three customized networks: reconstruction difficulty prediction (RDP), space-variant degradation estimation (SDE), and degradation and difficulty-informed reconstruction (DDR). Specifically, RDP learns the reconstruction difficulty with the proposed reconstruction-distance supervision. Then, SDE is designed to estimate space-variant degradation kernels according to the difficulty map. Finally, both degradation kernels and reconstruction difficulty are fed into DDR, which takes into account such two prior knowledge information to guide super-resolution (SR).
                                            </p>
                                            <span class="fst-italic">IEEE Transactions on Neural Networks and Learning Systems, 2024</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始5-->

                                <!--单个论文开始6-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/PRDet.jpg" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>Pareto Refocusing for Drone-View Object Detection</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/9905640" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng; Mengjingcheng Mo; Yinghua Zhou; Chenqiang Gao; Weisheng Li; Xinbo Gao*
                                            <hr class="divider">
                                            <p>
                                                Drone-view Object Detection (DOD) is a meaningful but challenging task. It hits a bottleneck due to two main reasons: (1) The high proportion of difficult objects (e.g., small objects, occluded objects, etc.) makes the detection performance unsatisfactory. (2) The unevenly distributed objects make detection inefficient. These two factors also lead to a phenomenon, obeying the Pareto principle, that some challenging regions occupying a low area proportion of the image have a significant impact on the final detection while the vanilla regions occupying the major area have a negligible impact due to the limited room for performance improvement. Motivated by the human visual system that naturally attempts to invest unequal energies in things of hierarchical difficulty for recognizing objects effectively, this paper presents a novel Pareto Refocusing Detection (PRDet) network that distinguishes the challenging regions from the vanilla regions under reverse-attention guidance and refocuses the challenging regions with the assistance of the region-specific context. Specifically, we first propose a Reverse-attention Exploration Module (REM) that excavates the potential position of difficult objects by suppressing the features which are salient to the commonly used detector. Then, we propose a Region-specific Context Learning Module (RCLM) that learns to generate specific contexts for strengthening the understanding of challenging regions. It is noteworthy that the specific context is not shared globally but unique for each challenging region with the exploration of spatial and appearance cues. 
                                            </p>
                                            <span class="fst-italic">IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2022</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始6-->
                                
                                <!--单个论文开始7-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/CrossNet.jpg" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>CrossNet: Detecting Objects as Crosses</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/9357941" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng*; Ying Liu; Zhihui Wang; Haibo Hu; Xinbo Gao
                                            <hr class="divider">
                                            <p>
                                                With the use of deep learning, object detection has achieved great breakthroughs. However, existing object detection methods still can not cope with challenging environments, such as dense objects, small objects, and object scale variations. To address these issues, this paper proposes a novel keypoint-based detection framework, called CrossNet, which significantly improves detection performance with minimal costs. In our approach, an object is modeled as a cross that consists of a center keypoint and a specific size, which eliminates the need of hand-craft anchor design. The proposed CrossNet outputs three types of maps: the center map, size map, and offset map, where both center map and offset map are to predict the center keypoints of objects and the size map is to estimate the sizes (width and height) of objects. Specifically, we first design a cascaded center prediction method that introduces a coarse-to-fine idea to improve center prediction. Furthermore, since center prediction considered as a classification task is easier than size regression relatively, we design a center-attention size regression module that uses the detection results of centers to assist the size prediction. In addition, a slightly modified hourglass network is designed to enhance the quality of feature maps for center and size prediction.
                                            </p>
                                            <span class="fst-italic">IEEE Transactions on Multimedia (IEEE TMM), 2021</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始7-->

                                <!--单个论文开始8-->
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-start">
                                        <div class="col-lg-3 col-sm-3">
                                            <img src="./paper_img/2020_TITS.jpg" alt="Zoom Image" class="zoom-image">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between pb-2" style="margin-bottom: -15px;">
                                                <span>Robust Obstacle Detection and Recognition for Driver Assistance Systems</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://ieeexplore.ieee.org/abstract/document/8691693" target="_blank">
                                                    <img src="img/paper_link.png" alt="Paper Link" width="20" height="20">
                                                </a>
                                            </h5>
                                            Jiaxu Leng; Ying Liu*; Dawei Du; Tianlin Zhang; Pei Quan
                                            <hr class="divider">
                                            <p>
                                                This paper proposes a robust obstacle detection and recognition method for driver assistance systems. Unlike existing methods, our method aims to detect and recognize obstacles on the road rather than all the obstacles in the view. The proposed method involves two stages aiming at an increased quality of the results. The first stage is to locate the positions of obstacles on the road. In order to accurately locate the on-road obstacles, we propose an obstacle detection method based on the U-V disparity map generated from a stereo vision system. The proposed U-V disparity algorithm makes use of the V-disparity map that provides a good representation of the geometric content of the road region to extract the road features, and then detects the on-road obstacles using our proposed realistic U-disparity map that eliminates the foreshortening effects caused by the perspective projection of pinhole imaging. The proposed realistic U-disparity map greatly improves the detection accuracy of the distant obstacles compared with the conventional U-disparity map. Second, the detection results of our proposed U-V disparity algorithm are put into a context-aware Faster-RCNN that combines the interior and contextual features to improve the recognition accuracy of small and occluded obstacles. Specifically, we propose a context-aware module and apply it into the architecture of Faster-RCNN. 
                                            </p>
                                            <span class="fst-italic">IEEE Transactions on Intellig Transportation Systems(IEEE TITS), 2020</span>
                                        </div>
                                    </div>
                                </div>
                                 <!--单个论文开始8-->
                            </div>
                        </div>

                        <div id="tab-2" class="tab-pane fade show p-0">
                            <div class="row g-4">
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/bg-hero_van.jpg" alt=""">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <a class="btn btn-square btn-primary mx-1" href="" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/bg-hero_van.jpg" alt=""">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <a class="btn btn-square btn-primary mx-1" href="" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div id="tab-3" class="tab-pane fade show p-0">
                            <div class="row g-4">
                                <div class="col-lg-6">
                                    <div class="d-flex align-items-center">
                                        <img class="flex-shrink-0 img-fluid rounded" src="img/applet.png" alt="" style="width: 120px;">
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Weixin Applet</span>
                                                <span class="text-primary"><i class="fab fa-weixin"></i></span>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-6">
                                    <div class="d-flex align-items-center">
                                        <img class="flex-shrink-0 img-fluid rounded" src="img/bird.jpg" alt="" style="width: 80px;">
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <span class="text-primary">$115</span>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-6">
                                    <div class="d-flex align-items-center">
                                        <img class="flex-shrink-0 img-fluid rounded" src="img/bird.jpg" alt="" style="width: 80px;">
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Chicken Burger</span>
                                                <span class="text-primary">$115</span>
                                            </h5>
                                            <small class="fst-italic">Ipsum ipsum clita erat amet dolor justo diam</small>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>



                        <div id="tab-4" class="tab-pane fade show p-0">
                            <div class="row g-4">
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/iip_black_h.png" alt=""">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>IIP Logos</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://github.com/OpenTVI/OpenTVI.github.io" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Different versions of our logos, for uses under different scenarios.  </small>
                                            <a class="btn btn-square btn-primary mx-1" href="resources/iip-logo.zip"><i class="fa fa-download"></i></a>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/xdu_red_black.png" alt="">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>XDU Logos</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://github.com/note286/xdulogo" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic">Different versions of XDU logos, for uses under different scenarios.  </small>
                                            <a class="btn btn-square btn-primary mx-1" href="resources/xdu-logo.zip"><i class="fa fa-download"></i></a>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-12">
                                    <div class="d-flex align-items-center">
                                        <div class="col-lg-3 col-sm-3">
                                            <img class="flex-shrink-0 img-fluid rounded" src="img/ppt-iip.png" alt="">
                                        </div>
                                        <div class="w-100 d-flex flex-column text-start ps-4">
                                            <h5 class="d-flex justify-content-between border-bottom pb-2">
                                                <span>Templetes</span>
                                                <a class="btn btn-square btn-primary mx-1" href="https://github.com/iip-xdu/iip-template" target="_blank"><i class="fab fa-github"></i></a>
                                            </h5>
                                            <small class="fst-italic"> Slides templates (16:9 PPT) with our logos.</small>
                                            <a class="btn btn-square btn-primary mx-1" href="https://github.com/iip-xdu/iip-template" target="_blank"><i class="fa fa-download"></i></a>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>


                    </div>
                </div>
            </div>
        </div>
        <!-- Menu End -->


        <!-- Footer Start -->
<!--         <div class="container-fluid bg-dark text-light footer pt-5 mt-5 wow fadeIn" data-wow-delay="0.1s">
            <div class="container py-5">
                <div class="row g-5">
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Laboratory</h4>
                        <a class="btn btn-link" href="index.html">About Us</a>
                        <a class="btn btn-link" href="contact.html">Contact Us</a>
                        <a class="btn btn-link" href="">Reservation</a>
                        <a class="btn btn-link" href="">Privacy Policy</a>
                        <a class="btn btn-link" href="">Terms & Condition</a>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Contact</h4>
                        <p class="mb-2"><i class="fa fa-map-marker-alt me-3"></i>No. 2 Chongwen Road, Nan'an District, Chongqing, </p>
                        <p class="mb-2"><i class="fa fa-globe me-3"></i>CQUPT, ChongQing 10617, China</p>
                        <p class="mb-2"><i class="fa fa-envelope me-3"></i>lengjx <i class="fa fa-at"></i> cqupt.edu.cn</p>
                        <div class="d-flex pt-2">
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-weixin"></i></a>
                            <a class="btn btn-outline-light btn-social" href="https://github.com/OpenTVI/OpenTVI.github.io" target="_blank"><i class="fab fa-github"></i></a>

                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Links</h4>
                        <a class="btn btn-link" href="https://www.cqupt.edu.cn/" target="_blank">CQUPT</a>
                        <a class="btn btn-link" href="https://see.xidian.edu.cn/faculty/xbgao/" target="_blank">VIPSL (Xinbo Gao) </a>
                        <a class="btn btn-link" href="https://www.researchgate.net/profile/Jiaxu-Leng-2" target="_blank">Home (Jiaxu Leng) </a>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Pages</h4>
                      
                        <a class="btn btn-link" href="index.html" class="nav-item nav-link">Home</a>
                        <a class="btn btn-link" href="team.html" class="nav-item nav-link">Team</a>
                        <a class="btn btn-link" href="research.html" class="nav-item nav-link">Research</a>
                        <a class="btn btn-link" href="resources.html" class="nav-item nav-link">Resources</a>
                        <a class="btn btn-link" href="contact.html" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="copyright">
                    <div class="row">
                        <div class="col-md-6 text-center text-md-start mb-3 mb-md-0">
                            &copy; <a class="border-bottom" href="#">TVI Group.</a>, All Right Reserved.

              Designed By <a class="border-bottom" href="https://htmlcodex.com">HTML Codex</a>
                        </div>
                        <div class="col-md-6 text-center text-md-end">
                            <div class="footer-menu">
                                <a href="">Home</a>
                                <a href="">Cookies</a>
                                <a href="">Help</a>
                                <a href="">FQAs</a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div> -->
                <div class="container-fluid bg-dark text-light footer pt-5 mt-5 wow fadeIn" data-wow-delay="0.1s">
            <div class="container py-5">
                <div class="row g-5">
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Laboratory</h4>

                        <a class="btn btn-link" href="index.html" class="nav-item nav-link">About Us</a>
                        <a class="btn btn-link" href="team.html" class="nav-item nav-link">Team Members</a>
                        <a class="btn btn-link" href="research.html" class="nav-item nav-link">Research Topics</a>
                        <a class="btn btn-link" href="resources.html" class="nav-item nav-link">Resources</a>
                        <a class="btn btn-link" href="contact.html" class="nav-item nav-link">Contact Us</a>
                        <!--
                        <a class="btn btn-link" href="">Reservation</a>
                        <a class="btn btn-link" href="">Privacy Policy</a>
                        <a class="btn btn-link" href="">Terms & Condition</a>
                        <img class="flex-shrink-0 img-fluid rounded" src="img/iip_white2.png" alt="" style="width: 200px;">
                      -->

                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Contact</h4>
                        <p class="mb-2"><i class="fa fa-map-marker-alt me-3"></i>No. 2 Chongwen Road, Nan'an District, Chongqing, </p>
                        <p class="mb-2"><i class="fa fa-globe me-3"></i>CQUPT, ChongQing 10617, China</p>
                        <p class="mb-2"><i class="fa fa-envelope me-3"></i>lengjx <i class="fa fa-at"></i> cqupt.edu.cn</p>
                        <div class="d-flex pt-2">
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-weixin"></i></a>
                            <a class="btn btn-outline-light btn-social" href="https://github.com/OpenTVI/OpenTVI.github.io" target="_blank"><i class="fab fa-github"></i></a>
                            <!--
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-youtube"></i></a>
                            <a class="btn btn-outline-light btn-social" href=""><i class="fab fa-linkedin-in"></i></a>
                          -->
                        </div>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Links</h4>
                        <a class="btn btn-link" href="https://www.cqupt.edu.cn/" target="_blank">CQUPT</a>
                        <a class="btn btn-link" href="https://see.xidian.edu.cn/faculty/xbgao/" target="_blank">VIPSL (Xinbo Gao) </a>
                        <a class="btn btn-link" href="https://www.researchgate.net/profile/Jiaxu-Leng-2" target="_blank">Home (Jiaxu Leng) </a>
                    </div>
                    <div class="col-lg-3 col-md-6">
                        <h4 class="section-title ff-secondary text-start text-primary fw-normal mb-4">Pages</h4>
                        <!--
                        <p>You can try our work.</p>
                        <div class="position-relative mx-auto" style="max-width: 400px;">
                            <input class="form-control border-primary w-100 py-3 ps-4 pe-5" type="text" placeholder="Your email">
                            <button type="button" class="btn btn-primary py-2 position-absolute top-0 end-0 mt-2 me-2">SignUp</button>
                        </div>
                        <img class="img-fluid flex-shrink-0 rounded-circle" src="img/applet.png" style="width: 50px; height: 50px;">
                      -->
                        <a class="btn btn-link" href="index.html" class="nav-item nav-link">Home</a>
                        <a class="btn btn-link" href="team.html" class="nav-item nav-link">Team</a>
                        <a class="btn btn-link" href="research.html" class="nav-item nav-link">Research</a>
                        <a class="btn btn-link" href="resources.html" class="nav-item nav-link">Resources</a>
                        <a class="btn btn-link" href="contact.html" class="nav-item nav-link">Contact</a>
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="copyright">
                    <div class="row">
                        <div class="col-md-6 text-center text-md-start mb-3 mb-md-0">
                            &copy; <a class="border-bottom" href="#">TVI Group.</a>, All Right Reserved.

							<!--/*** This template is free as long as you keep the footer author’s credit link/attribution link/backlink. If you'd like to use the template without the footer author’s credit link/attribution link/backlink, you can purchase the Credit Removal License from "https://htmlcodex.com/credit-removal". Thank you for your support. ***/-->
							Designed By <a class="border-bottom" href="https://htmlcodex.com">HTML Codex</a>
                        </div>
                        <div class="col-md-6 text-center text-md-end">
                            <div class="footer-menu">
                                <a href="">Home</a>
                                <a href="">Cookies</a>
                                <a href="">Help</a>
                                <a href="">FQAs</a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Footer End -->


        <!-- Back to Top -->
        <a href="#" class="btn btn-lg btn-primary btn-lg-square back-to-top"><i class="bi bi-arrow-up"></i></a>
    </div>

    <!-- JavaScript Libraries -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"></script>
    <script src="lib/wow/wow.min.js"></script>
    <script src="lib/easing/easing.min.js"></script>
    <script src="lib/waypoints/waypoints.min.js"></script>
    <script src="lib/counterup/counterup.min.js"></script>
    <script src="lib/owlcarousel/owl.carousel.min.js"></script>
    <script src="lib/tempusdominus/js/moment.min.js"></script>
    <script src="lib/tempusdominus/js/moment-timezone.min.js"></script>
    <script src="lib/tempusdominus/js/tempusdominus-bootstrap-4.min.js"></script>

    <!-- Template Javascript -->
    <script src="js/main.js"></script>
</body>

</html>
